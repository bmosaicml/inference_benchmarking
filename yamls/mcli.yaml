integrations:
- integration_type: git_repo
  git_repo: bmosaicml/inference_benchmarking
  git_branch: inference_benchmarking

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `examples/llm/README.md`
# to convert and host the full 'train' dataset.
command: |
  cd inference_benchmarking
  pip install -r requirements.txt
  composer inference.py /mnt/config/parameters.yaml
image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
optimization_level: 0

# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME
run_name: hf_accelerate

gpu_num: 8
gpu_type: a100_80gb
cluster: r1z1

# The below is injected as a YAML file: /mnt/config/parameters.yaml
# but is not used in this example.
parameters:
  # Tokenizer
  tokenizer:
    type: hftokenizer
    args:
      tokenizer_name: gpt2
      max_seq_len: 2048

  model:
    model_name: EleutherAI/gpt-neo-125M
    
  inference_platform: accelerate
  dtype: float32 
  num_tokens: 100
  batch_size: 64

  input_sentences: 
    - "DeepSpeed is a machine learning framework"
    -  "He is working on"
    -  "He has a"
    -  "He got all"
    -  "Everyone is happy and I can"
    -  "The new movie that got Oscar this year"
    -  "In the far far distance from our galaxy,"
    -  "Peace is the only way"

